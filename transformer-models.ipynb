{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb70bcd2-357f-41a0-ac95-12b273ead7f4",
   "metadata": {},
   "source": [
    "# Working with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97337b-6ae3-4ef8-93a4-30c22e7a1a4a",
   "metadata": {},
   "source": [
    "Some examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5b4f5a-0935-4df9-a161-c1ac61c9b351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/anaconda3/envs/hf_nlp_course/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0836967ba602464e9d933c3e3a13c297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c75d74-9adc-4284-9ce0-66a6d9883f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1f6b7f-fec7-47c2-aae1-813f06d1af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4705215d19444bab9b41f377d8adb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445940017700195, 0.11197788268327713, 0.043428145349025726]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278033c9-6b32-4475-9f97-4f374b156ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624bd7af9d534e65bca0baa014cab11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9850e39e389444beb718da29ff73d287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b84fdd035224d60938633e85974ee8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You will learn how dark the world is today, how to identify darkness in this way,\" he told The Post. \"And it is the most important of all the things this human species has become, and I know that from every aspect of our lives'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"You will learn how dark the world is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b73c24-5985-45d3-991e-f749942ed9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f6d5bfc82e4203879384394945b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511a1cc1c4974f68b4b5a4392aa9a0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f36e5e7a74bce99e6f15ba6fdc918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I hate everything about the president, he's an authoritarian, a misogynist, a racist, a misogynist.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation with a specific model\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"I hate everything about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0383adab-c0e3-4691-8fa4-669085c32ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           TextGenerationPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x32b35bac0>\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/hf_nlp_course/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
       "specified text prompt.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       ">>> generator = pipeline(model=\"gpt2\")\n",
       ">>> generator(\"I can't believe you did such a \", do_sample=False)\n",
       "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
       "\n",
       ">>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
       ">>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
       "```\n",
       "\n",
       "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
       "\n",
       "This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
       "`\"text-generation\"`.\n",
       "\n",
       "The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
       "objective, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models\n",
       "on [huggingface.co/models](https://huggingface.co/models?filter=text-generation).\n",
       "\n",
       "Arguments:\n",
       "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
       "    tokenizer ([`PreTrainedTokenizer`]):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        [`PreTrainedTokenizer`].\n",
       "    modelcard (`str` or [`ModelCard`], *optional*):\n",
       "        Model card attributed to the model for this pipeline.\n",
       "    framework (`str`, *optional*):\n",
       "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
       "        installed.\n",
       "\n",
       "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
       "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
       "        provided.\n",
       "    task (`str`, defaults to `\"\"`):\n",
       "        A task-identifier for the pipeline.\n",
       "    num_workers (`int`, *optional*, defaults to 8):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
       "        workers to be used.\n",
       "    batch_size (`int`, *optional*, defaults to 1):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
       "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
       "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
       "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
       "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
       "    device (`int`, *optional*, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id. You can pass native `torch.device` or a `str` too.\n",
       "    binary_output (`bool`, *optional*, defaults to `False`):\n",
       "        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Complete the prompt(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    args (`str` or `List[str]`):\n",
       "        One or several prompts (or one list of prompts) to complete.\n",
       "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to\n",
       "        `True`, the decoded text is not returned.\n",
       "    return_text (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to return the decoded texts in the outputs.\n",
       "    return_full_text (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `False` only added text is returned, otherwise the full text is returned. Only meaningful if\n",
       "        *return_text* is set to True.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to clean up the potential extra spaces in the text output.\n",
       "    prefix (`str`, *optional*):\n",
       "        Prefix added to prompt.\n",
       "    handle_long_generation (`str`, *optional*):\n",
       "        By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
       "        the model maximum length). There is no perfect way to adress this (more info\n",
       "        :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
       "        strategies to work around that problem depending on your use case.\n",
       "\n",
       "        - `None` : default strategy where nothing in particular happens\n",
       "        - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
       "          truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
       "\n",
       "    generate_kwargs:\n",
       "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
       "        corresponding to your framework [here](./model#generative-models)).\n",
       "\n",
       "Return:\n",
       "    A list or a list of list of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
       "    of both `generated_text` and `generated_token_ids`):\n",
       "\n",
       "    - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
       "    - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
       "      ids of the generated text."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9650087a-ebc7-401f-9f86-c46662c72860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc50a7fc083b43f6a4adf857149e4a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8811657ede4bd485d76d205a2a9ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9ad6d960884d1281b23e6529357518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1b5641509f4ef19c23b0fb7ed7c72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1316932308410d9f743d58d800fd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d8559c1d21451bb56d94cd9fad58b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04318477585911751,\n",
       "  'token': 3116,\n",
       "  'token_str': ' write',\n",
       "  'sequence': 'I will teach you how to write.'},\n",
       " {'score': 0.026884425431489944,\n",
       "  'token': 1532,\n",
       "  'token_str': ' learn',\n",
       "  'sequence': 'I will teach you how to learn.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask filling\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"I will teach you how to <mask>.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac5a691-1ec8-4eae-bdd9-ba19c3bd5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b9977d90ba469d938dbcd0ffbd0d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df263fff4eb44367b038445c351eda51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9896630644798279,\n",
       " 'start': 30,\n",
       " 'end': 51,\n",
       " 'answer': 'Hardknocks Boxing Gym'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question answering\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\", \n",
    "    context=\"My name is Jack and I work at Hardknocks Boxing Gym as a boxing coach\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad0bc7f-c1fd-47de-a548-848931682e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6233571171760559, 'start': 57, 'end': 69, 'answer': 'boxing coach'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(\n",
    "    question=\"What do I work as?\", \n",
    "    context=\"My name is Jack and I work at Hardknocks Boxing Gym as a boxing coach\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d32f349-9967-406e-91ed-d73945a192d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" The brightest elliptical galaxies often have low-density cores that are not well described by Sérsic's law . Core-Sérsics family of models was introduced[12] to describe such galaxies . The Einasto profile is mathematically identical to the model, except that I is replaced by ρ, the volume density .\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"The brightest elliptical galaxies often have low-density cores that are not well described by Sérsic's law. The core-Sérsic family of models was introduced[12][13][14] to describe such galaxies. Core-Sérsic models have an additional set of parameters that describe the core.\n",
    "\n",
    "Dwarf elliptical galaxies and bulges often have point-like nuclei that are also not well described by Sérsic's law. These galaxies are often fit by a Sérsic model with an added central component representing the nucleus.[15][16]\n",
    "\n",
    "The Einasto profile is mathematically identical to the Sérsic profile, except that I is replaced by ρ, the volume density, and R is replaced by r, the internal (not projected on the sky) distance from the center.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e68427-4775-43d1-90a7-f36d2adbd27e",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3edca-2c1b-4954-b154-11d69a4c9fa7",
   "metadata": {},
   "source": [
    "Transformers are models trained on large datasets for some purpose. These models can then be fine-tuned for some other purpose with a smaller, domain-specific dataset, and the resulting model generally performs just about as well as a model that was trained from scratch. \n",
    "\n",
    "The general architecture of a transformer consists of an encoder and a decoder. The encoder develops an understanding of the input. The decoder receives input from the encoder, along with other inputs, and generates an output. Transformers use attention layers during the learning process, which give them partial or full access to the training dataset, based on the purpose for which the model is being trained. \n",
    "\n",
    "Encoder models develop an understanding of the entire input. Their attention layer allows the model to access all the words in the input. They are trained by somehow corrupting the initial input by masking a word or words in the input, and making the model predict or find the actual sentence. They are generally good at tasks such as sentence classification, named entity recognition and question answering. Examples of encoder models include BERT, ALBERT, ELECTRA and DistilBERT.\n",
    "\n",
    "Decoder models can predict the next tokens in a sequence. The attention layers in these models only allows the model access to tokens preceeding a given token. Their training process involves learning to predict the next word in a sentence. They are used for text generation. Examples models include GPT, CTRL and TransformerXL.  \n",
    "\n",
    "Encoder-decoder models are a special sort of sequence-to-sequence models where the attention layer of the encoder allows the model access to the entire sentence, while the decoder attention layer only to words before a given word. These models are generally trained replacing words with a single mask and requiring the model to predict the masked words. These models are good at tasks such as summarization, translation, and generative question answering, and examples include BART and T5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c955074-ddfc-4a30-8666-9ea02b059301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
